{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By418SpSPKwV",
        "outputId": "607ec388-e4b8-45ff-8fe9-1b3a6bcaac69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUi6i-HWPdBR"
      },
      "source": [
        "1.2.1 Classification accuracy (average):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXd0cx6nQgUz",
        "outputId": "86a08d7c-1150-4492-fc8d-306450a762ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['config.py',\n",
              " 'evaluation.py',\n",
              " 'simple.py',\n",
              " 'save',\n",
              " '__pycache__',\n",
              " 'logs',\n",
              " 'utils.py',\n",
              " 'multi_models.py',\n",
              " 'Thesis Record.gdoc',\n",
              " 'Preprocessing_Multimodal.ipynb',\n",
              " 'Models_for_using.gdoc',\n",
              " 'scene-classification-images-and-audio.zip',\n",
              " 'scene_dataset.py',\n",
              " 'data',\n",
              " 'sampling.py',\n",
              " 'unimodal_image_adam_0.001best_checkpoint.pt',\n",
              " 'options.py',\n",
              " 'unimodal_image_sgd_0.001best_checkpoint.pt',\n",
              " 'multimodal_image_audio_sgd_0.001best_checkpoint.pt',\n",
              " 'multimodal_image_audio_adam_0.0001best_checkpoint.pt',\n",
              " 'multimodal_image_audio_sgd_0.0001best_checkpoint.pt',\n",
              " 'Copy of models.py',\n",
              " 'federated_main_unimodal.py',\n",
              " 'update.py',\n",
              " 'submissionAmaniCaioTest1.csv',\n",
              " 'submissionAmaniCaioTest.csv',\n",
              " 'get_data.py',\n",
              " 'federated_main_multimodal.py',\n",
              " 'models.py',\n",
              " 'multimodal_image_audio_adam_0.001best_checkpoint.pt',\n",
              " 'Scene-Classification.ipynb',\n",
              " 'Evaluation.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "path = '/content/drive/My Drive/Scene-Classification'\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "# sys.path.append('')\n",
        "# ! python options.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTAGZQuPQwOp",
        "outputId": "777b8fa9-54d4-4624-ce42-7979cd4984b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG_PWghBQzN5",
        "outputId": "bf3490a0-41e4-4437-b017-a71f1455aac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/My Drive/Scene-Classification/config.py'\n",
            "'/content/drive/My Drive/Scene-Classification/Copy of models.py'\n",
            "'/content/drive/My Drive/Scene-Classification/evaluation.py'\n",
            "'/content/drive/My Drive/Scene-Classification/federated_main_multimodal.py'\n",
            "'/content/drive/My Drive/Scene-Classification/federated_main_unimodal.py'\n",
            "'/content/drive/My Drive/Scene-Classification/get_data.py'\n",
            "'/content/drive/My Drive/Scene-Classification/models.py'\n",
            "'/content/drive/My Drive/Scene-Classification/multi_models.py'\n",
            "'/content/drive/My Drive/Scene-Classification/options.py'\n",
            "'/content/drive/My Drive/Scene-Classification/sampling.py'\n",
            "'/content/drive/My Drive/Scene-Classification/scene_dataset.py'\n",
            "'/content/drive/My Drive/Scene-Classification/simple.py'\n",
            "'/content/drive/My Drive/Scene-Classification/update.py'\n",
            "'/content/drive/My Drive/Scene-Classification/utils.py'\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('My Drive/Colab Notebooks/Scene-Classification')\n",
        "import options\n",
        "!ls /content/drive/My\\ Drive/Scene-Classification/*.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhHxopDiQ1u9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "from tensorboardX import SummaryWriter\n",
        "from options import args_parser\n",
        "from update import LocalUpdate, test_inference,multi_test_inference\n",
        "from models import vgg_16, mlp_fusion\n",
        "import get_data #, scene_dataset\n",
        "from scene_dataset import ImageDataset, AudioDataset, ImageAudioDataset\n",
        "# from utils import get_dataset, average_weights, exp_details\n",
        "# from videosets import VideoFrameDataset, ImglistToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4EIyey6Sdwb"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import preprocessing\n",
        "def dataset_card(condition):\n",
        "    class_list = {'FOREST': 0, 'CITY': 1, 'BEACH': 2, 'CLASSROOM': 3, 'RIVER': 4, 'JUNGLE': 5, 'RESTAURANT': 6,\n",
        "                  'GROCERY-STORE': 7, 'FOOTBALL-MATCH': 8}\n",
        "    datadir = Path.cwd() / 'data'\n",
        "    if condition == 'image':\n",
        "        # train data\n",
        "        data_df_train = pd.read_csv(os.path.join(datadir, 'train.csv'), delimiter=',', nrows=None)\n",
        "        data_df_train['CLASS2'] = data_df_train['CLASS2'].map(class_list)\n",
        "        train_data = np.array(data_df_train)\n",
        "        train_img_list = data_df_train['IMAGE']\n",
        "        train_labels = train_data[:, -1].astype('int32')\n",
        "        # test data\n",
        "        data_df_test = pd.read_csv(os.path.join(datadir, 'test.csv'), delimiter=',', nrows=None)\n",
        "        data_df_test['CLASS2'] = data_df_test['CLASS2'].map(class_list)\n",
        "        test_data = np.array(data_df_test)\n",
        "        test_img_list = data_df_test['IMAGE']\n",
        "        test_labels = test_data[:, -1].astype('int32')\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize(size=(224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "\n",
        "        ])\n",
        "        test_transform = transforms.Compose([\n",
        "            # transforms.Grayscale(),\n",
        "            transforms.Resize(size=(224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        # train_dataset = datasets.ImageFolder('data/CXR/train', transform=train_transform)\n",
        "        train_dataset = ImageDataset(root_dir=datadir,\n",
        "                                    files=train_img_list,\n",
        "                                    labels=train_labels,\n",
        "                                    img_transform=train_transform)\n",
        "        test_dataset = ImageDataset(root_dir=datadir,\n",
        "                                    files=test_img_list,\n",
        "                                    labels=test_labels,\n",
        "                                    img_transform=test_transform)\n",
        "\n",
        "    if condition == 'image_audio':\n",
        "        # train data\n",
        "        data_df_train = pd.read_csv(os.path.join(datadir, 'train.csv'), delimiter=',', nrows=None)\n",
        "        data_df_train['CLASS2'] = data_df_train['CLASS2'].map(class_list)\n",
        "        train_data = np.array(data_df_train)\n",
        "        train_img_list = data_df_train['IMAGE']\n",
        "        train_audio = train_data[:, 1:-2].astype('float32')\n",
        "        train_labels = train_data[:, -1].astype('int32')\n",
        "        scaler_train = preprocessing.StandardScaler().fit(train_audio)\n",
        "        audio_train_scaled = scaler_train.transform(train_audio)\n",
        "        # test data\n",
        "        data_df_test = pd.read_csv(os.path.join(datadir, 'test.csv'), delimiter=',', nrows=None)\n",
        "        data_df_test['CLASS2'] = data_df_test['CLASS2'].map(class_list)\n",
        "        test_data = np.array(data_df_test)\n",
        "        test_img_list = data_df_test['IMAGE']\n",
        "        test_audio = test_data[:, 1:-2].astype('float32')\n",
        "        test_labels = test_data[:, -1].astype('int32')\n",
        "        scaler_test = preprocessing.StandardScaler().fit(test_audio)\n",
        "        audio_test_scaled = scaler_test.transform(test_audio)\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize(size=(224, 224)),\n",
        "            # transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.Resize(size=(224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "        ])\n",
        "        audio_transform = None\n",
        "        train_dataset = ImageAudioDataset(root_dir=datadir,\n",
        "                                          files=train_img_list,\n",
        "                                          audio=audio_train_scaled,\n",
        "                                          labels=train_labels,\n",
        "                                          img_transform=train_transform,\n",
        "                                          audio_transform=audio_transform)\n",
        "        test_dataset = ImageAudioDataset(root_dir=datadir,\n",
        "                                          files=test_img_list,\n",
        "                                          audio=audio_test_scaled,\n",
        "                                          labels=test_labels,\n",
        "                                          img_transform=test_transform,\n",
        "                                          audio_transform=audio_transform)\n",
        "    #     test_dataset = datasets.ImageFolder('data/CT/test', transform=test_transform)\n",
        "        # vali_dataset = datasets.ImageFolder('data/CT/vali', transform=test_transform)\n",
        "        # train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=1)\n",
        "        # test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=1)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# def test_inference(model, test_dataset):\n",
        "#     \"\"\"\n",
        "#     Returns the test accuracy and loss.\n",
        "#     \"\"\"\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         loss, total, correct = 0.0, 0.0, 0.0\n",
        "#         pred_list = []\n",
        "#         y_true = []\n",
        "#         # device = 'cuda' if args.gpu else 'cpu'\n",
        "#         device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "#         criterion = nn.CrossEntropyLoss().to(device)\n",
        "#         testloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "#         for batch_idx, (images, labels) in enumerate(testloader):\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             # Inference\n",
        "#             outputs = model(images)\n",
        "#             batch_loss = criterion(outputs, labels)\n",
        "#             loss += batch_loss.item()\n",
        "#             # Prediction\n",
        "#             _, pred_labels = torch.max(outputs, 1)\n",
        "#             pred_labels = pred_labels.view(-1)\n",
        "#             correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "#             total += len(labels)\n",
        "#             pred_list.extend(pred_labels.cpu().numpy())\n",
        "#             y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "#         accuracy = correct / total\n",
        "#         loss = loss / len(testloader)\n",
        "#         f1 = f1_score(y_true, pred_list, average=None)\n",
        "#     return accuracy, loss, f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image modality for nine classes\n",
        "Train Accruray: 0.944859068183465 \\\\\n",
        "Loss: 0.17766076612665696 \\\\\n",
        "[0.97870412 0.91946993 0.99008711 0.97593097 0.91272727 0.97732325\n",
        " 0.87192324 0.92902833 0.94380231] 0.9443329490326295 \\\\\n",
        " ------------------------------------ \\\\\n",
        "Test Accuracy: 0.9327536231884058 \\\\\n",
        "Loss: 0.22084674558469228 \\\\\n",
        "[0.97794118 0.89878543 0.98806683 0.96350365 0.8960396  0.97183099\n",
        " 0.86378738 0.90167866 0.93890675] 0.9333933835288425\n",
        "### Audio modality for nine classes\n",
        "Train Accruray: 0.9704369248605174 \\\\\n",
        "Loss: 0.11190520517131383 \\\\\n",
        "[0.99493321 0.96381151 0.9740566  0.97456857 0.96302976 0.99866607\n",
        " 0.93766012 0.9740376  0.95762376] 0.9709319120795106 \\\\\n",
        "--------------------------------------- \\\\\n",
        "Test Accuracy: 0.936231884057971 \\\\\n",
        "Loss: 0.2986410068614142 \\\\\n",
        "[0.98141264 0.93697479 0.94859813 0.92673993 0.91262136 1.\n",
        " 0.83687943 0.94009217 0.95327103] 0.9373988302984038"
      ],
      "metadata": {
        "id": "S7ZIO5-01rfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "model = vgg_16(in_channels=3, num_classes=9)\n",
        "model.to(device)\n",
        "PATH_pth = '/content/drive/My Drive/Scene-Classification/unimodal_image_adam_0.001best_checkpoint.pt'\n",
        "state_dict = torch.load(PATH_pth)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "train_dataset, test_dataset = dataset_card('image')\n",
        "accuracy_train, loss_train, f1_train = test_inference(model, train_dataset)\n",
        "print(accuracy_train)\n",
        "print(loss_train)\n",
        "print(f1_train, sum(f1_train)/len(f1_train))\n",
        "print('------------------------------------')\n",
        "accuracy_test, loss_test, f1_test = test_inference(model, test_dataset)\n",
        "print(accuracy_test)\n",
        "print(loss_test)\n",
        "print(f1_test, sum(f1_test)/len(f1_test))"
      ],
      "metadata": {
        "id": "reqVxify1ntA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e0effc-64f6-4bb8-ebb3-8b9b1520c38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.944859068183465\n",
            "0.17766076612665696\n",
            "[0.97870412 0.91946993 0.99008711 0.97593097 0.91272727 0.97732325\n",
            " 0.87192324 0.92902833 0.94380231] 0.9443329490326295\n",
            "------------------------------------\n",
            "0.9327536231884058\n",
            "0.22084674558469228\n",
            "[0.97794118 0.89878543 0.98806683 0.96350365 0.8960396  0.97183099\n",
            " 0.86378738 0.90167866 0.93890675] 0.9333933835288425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio modality for nine classes\n"
      ],
      "metadata": {
        "id": "MqqCkYWmPoS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "model = mlp_fusion()\n",
        "model.to(device)\n",
        "PATH_pth = '/content/drive/My Drive/Scene-Classification/multimodal_image_audio_adam_0.001best_checkpoint.pt'\n",
        "state_dict = torch.load(PATH_pth)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "train_dataset, test_dataset = dataset_card('image_audio')\n",
        "accuracy_train, loss_train, f1_train = multi_test_inference(model, train_dataset)\n",
        "print(accuracy_train)\n",
        "print(loss_train)\n",
        "print(f1_train, sum(f1_train)/len(f1_train))\n",
        "print('------------------------------------')\n",
        "accuracy_test, loss_test, f1_test = multi_test_inference(model, test_dataset)\n",
        "print(accuracy_test)\n",
        "print(loss_test)\n",
        "print(f1_test, sum(f1_test)/len(f1_test))"
      ],
      "metadata": {
        "id": "XkEaMo16oGgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247,
          "referenced_widgets": [
            "185257ac4e974e4082994e1c6a1b226e",
            "2bc88ffd0fe6424387333c6ce8ef882f",
            "cbbcb3dc73624e4dafb7ad3894ac1b3c",
            "3d5955b910744fdc9e232e679e78a767",
            "ef5bacaab826461ba4e3b45f30f454cc",
            "c12bfdeefc9f4abea84380ba7939bb2b",
            "24702c7889ea43d685c6bf61c13cecf1",
            "59b5648baf97491ba19de65c25f8c1e2",
            "70dc5431f8d64db7ae069308f7185d6c",
            "f5284f70b6134287bb0650a0ea128ef9",
            "f09b9429ac624b5f9ab3156ea1c4659d"
          ]
        },
        "outputId": "00dd6a4e-37d0-4866-8761-600d62889bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "185257ac4e974e4082994e1c6a1b226e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9706543004130136\n",
            "0.10956076052281316\n",
            "[0.99357208 0.96193416 0.9803805  0.97696477 0.96685578 0.9973357\n",
            " 0.92828861 0.96989568 0.96063618] 0.970651495079777\n",
            "------------------------------------\n",
            "0.9350724637681159\n",
            "0.2715472163898604\n",
            "[0.98540146 0.93004115 0.95486936 0.93430657 0.92344498 0.99646643\n",
            " 0.82962963 0.92807425 0.93416928] 0.9351559002076022\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Evaluation.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "185257ac4e974e4082994e1c6a1b226e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bc88ffd0fe6424387333c6ce8ef882f",
              "IPY_MODEL_cbbcb3dc73624e4dafb7ad3894ac1b3c",
              "IPY_MODEL_3d5955b910744fdc9e232e679e78a767"
            ],
            "layout": "IPY_MODEL_ef5bacaab826461ba4e3b45f30f454cc"
          }
        },
        "2bc88ffd0fe6424387333c6ce8ef882f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12bfdeefc9f4abea84380ba7939bb2b",
            "placeholder": "​",
            "style": "IPY_MODEL_24702c7889ea43d685c6bf61c13cecf1",
            "value": "100%"
          }
        },
        "cbbcb3dc73624e4dafb7ad3894ac1b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59b5648baf97491ba19de65c25f8c1e2",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70dc5431f8d64db7ae069308f7185d6c",
            "value": 553433881
          }
        },
        "3d5955b910744fdc9e232e679e78a767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5284f70b6134287bb0650a0ea128ef9",
            "placeholder": "​",
            "style": "IPY_MODEL_f09b9429ac624b5f9ab3156ea1c4659d",
            "value": " 528M/528M [00:02&lt;00:00, 227MB/s]"
          }
        },
        "ef5bacaab826461ba4e3b45f30f454cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c12bfdeefc9f4abea84380ba7939bb2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24702c7889ea43d685c6bf61c13cecf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59b5648baf97491ba19de65c25f8c1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70dc5431f8d64db7ae069308f7185d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5284f70b6134287bb0650a0ea128ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f09b9429ac624b5f9ab3156ea1c4659d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}